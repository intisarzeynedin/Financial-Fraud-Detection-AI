{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Model Training\n",
        "## Credit Card Fraud Detection\n",
        "\n",
        "This notebook trains multiple ML models using modules from src folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path('..')))\n",
        "from src.data_loader import DataLoader\n",
        "from src.models import FraudDetectionModels\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loading cleaned data from: d:\\h\\Financial Fraud Detection-AI\\transactions\\notebooks\\..\\data\\creditcard_cleaned.csv\n",
            "  - Cleaned dataset: 283,726 transactions (duplicates removed)\n",
            "\n",
            "✓ Preprocessing cleaned dataset for ML training...\n",
            "\n",
            "Preprocessing data...\n",
            "Features: 29\n",
            "Feature columns: ['V1', 'V2', 'V3', 'V4', 'V5']... (showing first 5)\n",
            "\n",
            "Train set: 226980 samples\n",
            "  - Fraud: 378 (0.17%)\n",
            "Test set: 56746 samples\n",
            "  - Fraud: 95 (0.17%)\n",
            "\n",
            "Scaling features...\n",
            "\n",
            "✓ Preprocessing complete:\n",
            "  Training set: 226,980 samples, 29 features\n",
            "  Test set: 56,746 samples, 29 features\n",
            "  Total features: 29\n",
            "\n",
            "  Training class distribution:\n",
            "    Normal (0): 226,602 (99.83%)\n",
            "    Fraud (1):  378 (0.17%)\n"
          ]
        }
      ],
      "source": [
        "# Initialize data loader\n",
        "data_loader = DataLoader(data_dir='../data')\n",
        "\n",
        "# Load cleaned data (from Data Cleaning notebook)\n",
        "cleaned_csv_path = Path('../data/creditcard_cleaned.csv')\n",
        "if cleaned_csv_path.exists():\n",
        "    print(f\"✓ Loading cleaned data from: {cleaned_csv_path.absolute()}\")\n",
        "    df_clean = pd.read_csv(cleaned_csv_path)\n",
        "    print(f\"  - Cleaned dataset: {len(df_clean):,} transactions (duplicates removed)\")\n",
        "else:\n",
        "    print(\"⚠ WARNING: Cleaned data not found!\")\n",
        "    print(\"Loading original data and removing duplicates...\")\n",
        "    csv_path = Path('../data/creditcard.csv')\n",
        "    df_original = data_loader.load_csv_data('creditcard.csv')\n",
        "    df_clean = df_original.drop_duplicates(keep='first')\n",
        "    print(f\"  - Removed {df_original.duplicated().sum():,} duplicates\")\n",
        "\n",
        "# Preprocess cleaned data (feature engineering already done in previous notebook)\n",
        "print(f\"\\n✓ Preprocessing cleaned dataset for ML training...\")\n",
        "X_train, X_test, y_train, y_test, feature_cols = data_loader.preprocess_data(\n",
        "    df_clean,  # Using cleaned dataset\n",
        "    target_col='Class',\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Preprocessing complete:\")\n",
        "print(f\"  Training set: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
        "print(f\"  Test set: {X_test.shape[0]:,} samples, {X_test.shape[1]} features\")\n",
        "print(f\"  Total features: {len(feature_cols)}\")\n",
        "print(f\"\\n  Training class distribution:\")\n",
        "print(f\"    Normal (0): {y_train.value_counts()[0]:,} ({y_train.value_counts(normalize=True)[0]*100:.2f}%)\")\n",
        "print(f\"    Fraud (1):  {y_train.value_counts()[1]:,} ({y_train.value_counts(normalize=True)[1]*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models initialized. Available models:\n",
            "  - Logistic Regression\n",
            "  - Decision Tree\n",
            "  - Random Forest\n",
            "  - XGBoost\n"
          ]
        }
      ],
      "source": [
        "# Initialize model trainer\n",
        "models = FraudDetectionModels(models_dir='../models')\n",
        "\n",
        "print(\"Models initialized. Available models:\")\n",
        "print(\"  - Logistic Regression\")\n",
        "print(\"  - Decision Tree\")\n",
        "print(\"  - Random Forest\")\n",
        "print(\"  - XGBoost\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Baseline Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Training Logistic Regression...\n",
            "==================================================\n",
            "Using class_weight=balanced\n",
            "Model saved to ..\\models\\logistic_regression.pkl\n"
          ]
        }
      ],
      "source": [
        "# Train Logistic Regression with class weights\n",
        "lr_model = models.logistic_regression(\n",
        "    X_train, y_train, \n",
        "    use_smote=False, \n",
        "    class_weight='balanced'\n",
        ")\n",
        "models.save_model('logistic_regression', lr_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Training Decision Tree...\n",
            "==================================================\n",
            "Using class_weight=balanced\n",
            "Model saved to ..\\models\\decision_tree.pkl\n"
          ]
        }
      ],
      "source": [
        "# Train Decision Tree\n",
        "dt_model = models.decision_tree(\n",
        "    X_train, y_train, \n",
        "    use_smote=False, \n",
        "    class_weight='balanced'\n",
        ")\n",
        "models.save_model('decision_tree', dt_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Advanced Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Training Random Forest...\n",
            "==================================================\n",
            "Using class_weight=balanced\n",
            "Model saved to ..\\models\\random_forest.pkl\n"
          ]
        }
      ],
      "source": [
        "# Train Random Forest\n",
        "rf_model = models.random_forest(\n",
        "    X_train, y_train, \n",
        "    use_smote=False, \n",
        "    class_weight='balanced',\n",
        "    tune_hyperparameters=False\n",
        ")\n",
        "models.save_model('random_forest', rf_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Training XGBoost...\n",
            "==================================================\n",
            "Using scale_pos_weight=599.48\n",
            "Model saved to ..\\models\\xgboost.pkl\n"
          ]
        }
      ],
      "source": [
        "# Train XGBoost\n",
        "xgb_model = models.xgboost_model(\n",
        "    X_train, y_train, \n",
        "    use_smote=False,\n",
        "    tune_hyperparameters=False\n",
        ")\n",
        "models.save_model('xgboost', xgb_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Scaler and Feature Columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved preprocessing artifacts for model inference:\n",
            "  - scaler.pkl (StandardScaler fitted on cleaned data)\n",
            "  - feature_columns.pkl (feature column names)\n",
            "  Location: d:\\h\\Financial Fraud Detection-AI\\transactions\\notebooks\\..\\models\n",
            "\n",
            "  These will be used by the Streamlit app for predictions\n"
          ]
        }
      ],
      "source": [
        "# Save scaler and feature columns for later use (Streamlit app)\n",
        "import joblib\n",
        "\n",
        "scaler = data_loader.get_scaler()\n",
        "models_dir = Path('../models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "joblib.dump(scaler, models_dir / 'scaler.pkl')\n",
        "joblib.dump(feature_cols, models_dir / 'feature_columns.pkl')\n",
        "\n",
        "print(\"✓ Saved preprocessing artifacts for model inference:\")\n",
        "print(f\"  - scaler.pkl (StandardScaler fitted on cleaned data)\")\n",
        "print(f\"  - feature_columns.pkl (feature column names)\")\n",
        "print(f\"  Location: {models_dir.absolute()}\")\n",
        "print(f\"\\n  These will be used by the Streamlit app for predictions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ML TRAINING SUMMARY\n",
            "======================================================================\n",
            "\n",
            "1. Data Pipeline:\n",
            "   ✓ Used cleaned dataset from 01b_DataCleaning.ipynb\n",
            "   ✓ Applied preprocessing from 02_Preprocessing_FeatureEngineering.ipynb\n",
            "   ✓ All models trained on cleaned, preprocessed data\n",
            "\n",
            "2. Training Data:\n",
            "   - Training samples: 226,980\n",
            "   - Test samples: 56,746\n",
            "   - Features: 29\n",
            "   - Class imbalance handled with class weights/SMOTE\n",
            "\n",
            "3. Models Trained:\n",
            "   ✓ Logistic Regression (with class weights)\n",
            "   ✓ Decision Tree (with class weights)\n",
            "   ✓ Random Forest (with class weights)\n",
            "   ✓ XGBoost (with scale_pos_weight)\n",
            "\n",
            "4. Models Saved:\n",
            "   - Location: ../models/\n",
            "   - All models ready for evaluation\n",
            "\n",
            "5. Next Steps:\n",
            "   - Run 04_ML_Evaluation.ipynb to evaluate all models\n",
            "   - Compare model performance metrics\n",
            "   - Select best model for production\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ML TRAINING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n1. Data Pipeline:\")\n",
        "print(f\"   ✓ Used cleaned dataset from 01b_DataCleaning.ipynb\")\n",
        "print(f\"   ✓ Applied preprocessing from 02_Preprocessing_FeatureEngineering.ipynb\")\n",
        "print(f\"   ✓ All models trained on cleaned, preprocessed data\")\n",
        "print(f\"\\n2. Training Data:\")\n",
        "print(f\"   - Training samples: {X_train.shape[0]:,}\")\n",
        "print(f\"   - Test samples: {X_test.shape[0]:,}\")\n",
        "print(f\"   - Features: {len(feature_cols)}\")\n",
        "print(f\"   - Class imbalance handled with class weights/SMOTE\")\n",
        "print(f\"\\n3. Models Trained:\")\n",
        "print(f\"   ✓ Logistic Regression (with class weights)\")\n",
        "print(f\"   ✓ Decision Tree (with class weights)\")\n",
        "print(f\"   ✓ Random Forest (with class weights)\")\n",
        "print(f\"   ✓ XGBoost (with scale_pos_weight)\")\n",
        "print(f\"\\n4. Models Saved:\")\n",
        "print(f\"   - Location: ../models/\")\n",
        "print(f\"   - All models ready for evaluation\")\n",
        "print(f\"\\n5. Next Steps:\")\n",
        "print(f\"   - Run 04_ML_Evaluation.ipynb to evaluate all models\")\n",
        "print(f\"   - Compare model performance metrics\")\n",
        "print(f\"   - Select best model for production\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
