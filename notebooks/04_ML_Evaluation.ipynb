{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Model Evaluation\n",
        "## Credit Card Fraud Detection\n",
        "\n",
        "This notebook performs comprehensive evaluation of trained ML models using the evaluator module from src folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path('..')))\n",
        "from src.data_loader import DataLoader\n",
        "from src.models import FraudDetectionModels\n",
        "from src.evaluator import ModelEvaluator\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loading cleaned data from: d:\\h\\Financial Fraud Detection-AI\\transactions\\notebooks\\..\\data\\creditcard_cleaned.csv\n",
            "  - Using cleaned dataset: 283,726 transactions\n",
            "\n",
            "✓ Preprocessing cleaned dataset for evaluation...\n",
            "\n",
            "Preprocessing data...\n",
            "Features: 29\n",
            "Feature columns: ['V1', 'V2', 'V3', 'V4', 'V5']... (showing first 5)\n",
            "\n",
            "Train set: 226980 samples\n",
            "  - Fraud: 378 (0.17%)\n",
            "Test set: 56746 samples\n",
            "  - Fraud: 95 (0.17%)\n",
            "\n",
            "Scaling features...\n",
            "\n",
            "✓ Test set prepared:\n",
            "  Test samples: 56,746\n",
            "  Test features: 29\n",
            "  Test fraud cases: 95 (0.17%)\n",
            "  Test normal cases: 56651 (99.83%)\n"
          ]
        }
      ],
      "source": [
        "# Load test data (using cleaned dataset for consistency)\n",
        "data_loader = DataLoader(data_dir='../data')\n",
        "\n",
        "# Load cleaned data (from Data Cleaning notebook)\n",
        "cleaned_csv_path = Path('../data/creditcard_cleaned.csv')\n",
        "if cleaned_csv_path.exists():\n",
        "    print(f\"✓ Loading cleaned data from: {cleaned_csv_path.absolute()}\")\n",
        "    df_clean = pd.read_csv(cleaned_csv_path)\n",
        "    print(f\"  - Using cleaned dataset: {len(df_clean):,} transactions\")\n",
        "else:\n",
        "    print(\"⚠ WARNING: Cleaned data not found!\")\n",
        "    print(\"Loading original data and removing duplicates...\")\n",
        "    csv_path = Path('../data/creditcard.csv')\n",
        "    df_original = data_loader.load_csv_data('creditcard.csv')\n",
        "    df_clean = df_original.drop_duplicates(keep='first')\n",
        "    print(f\"  - Removed {df_original.duplicated().sum():,} duplicates\")\n",
        "\n",
        "# Preprocess cleaned data (same as training)\n",
        "print(f\"\\n✓ Preprocessing cleaned dataset for evaluation...\")\n",
        "X_train, X_test, y_train, y_test, feature_cols = data_loader.preprocess_data(\n",
        "    df_clean,  # Using cleaned dataset\n",
        "    target_col='Class',\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Test set prepared:\")\n",
        "print(f\"  Test samples: {X_test.shape[0]:,}\")\n",
        "print(f\"  Test features: {X_test.shape[1]}\")\n",
        "print(f\"  Test fraud cases: {y_test.sum()} ({y_test.mean()*100:.2f}%)\")\n",
        "print(f\"  Test normal cases: {len(y_test) - y_test.sum()} ({(1-y_test.mean())*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from ..\\models\\decision_tree.pkl\n",
            "Model loaded from ..\\models\\logistic_regression.pkl\n",
            "Model loaded from ..\\models\\random_forest.pkl\n",
            "Model loaded from ..\\models\\xgboost.pkl\n",
            "Loaded 4 models:\n",
            "  - decision_tree\n",
            "  - logistic_regression\n",
            "  - random_forest\n",
            "  - xgboost\n"
          ]
        }
      ],
      "source": [
        "# Load trained models\n",
        "models = FraudDetectionModels(models_dir='../models')\n",
        "\n",
        "# Load all available models\n",
        "model_files = list(Path('../models').glob('*.pkl'))\n",
        "model_files = [f for f in model_files if f.stem not in ['scaler', 'feature_columns']]\n",
        "\n",
        "trained_models = {}\n",
        "for model_file in model_files:\n",
        "    model_name = model_file.stem\n",
        "    trained_models[model_name] = models.load_model(model_name)\n",
        "\n",
        "print(f\"Loaded {len(trained_models)} models:\")\n",
        "for name in trained_models.keys():\n",
        "    print(f\"  - {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluator initialized. Reports will be saved to ../reports/\n"
          ]
        }
      ],
      "source": [
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(reports_dir='../reports')\n",
        "print(\"Evaluator initialized. Reports will be saved to ../reports/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Evaluate All Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Evaluating Decision Tree...\n",
            "==================================================\n",
            "\n",
            "Metrics for Decision Tree:\n",
            "  Accuracy:  0.9952\n",
            "  Precision: 0.2277\n",
            "  Recall:    0.7789\n",
            "  F1-Score:  0.3524\n",
            "  ROC-AUC:   0.8886\n",
            "  PR-AUC:    0.4971\n",
            "\n",
            "Confusion Matrix:\n",
            "  TN: 56400, FP: 251\n",
            "  FN: 21, TP: 74\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      1.00      1.00     56651\n",
            "       Fraud       0.23      0.78      0.35        95\n",
            "\n",
            "    accuracy                           1.00     56746\n",
            "   macro avg       0.61      0.89      0.67     56746\n",
            "weighted avg       1.00      1.00      1.00     56746\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating Logistic Regression...\n",
            "==================================================\n",
            "\n",
            "Metrics for Logistic Regression:\n",
            "  Accuracy:  0.9750\n",
            "  Precision: 0.0558\n",
            "  Recall:    0.8737\n",
            "  F1-Score:  0.1049\n",
            "  ROC-AUC:   0.9648\n",
            "  PR-AUC:    0.6752\n",
            "\n",
            "Confusion Matrix:\n",
            "  TN: 55247, FP: 1404\n",
            "  FN: 12, TP: 83\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      0.98      0.99     56651\n",
            "       Fraud       0.06      0.87      0.10        95\n",
            "\n",
            "    accuracy                           0.98     56746\n",
            "   macro avg       0.53      0.92      0.55     56746\n",
            "weighted avg       1.00      0.98      0.99     56746\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating Random Forest...\n",
            "==================================================\n",
            "\n",
            "Metrics for Random Forest:\n",
            "  Accuracy:  0.9995\n",
            "  Precision: 0.9853\n",
            "  Recall:    0.7053\n",
            "  F1-Score:  0.8221\n",
            "  ROC-AUC:   0.9299\n",
            "  PR-AUC:    0.8076\n",
            "\n",
            "Confusion Matrix:\n",
            "  TN: 56650, FP: 1\n",
            "  FN: 28, TP: 67\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      1.00      1.00     56651\n",
            "       Fraud       0.99      0.71      0.82        95\n",
            "\n",
            "    accuracy                           1.00     56746\n",
            "   macro avg       0.99      0.85      0.91     56746\n",
            "weighted avg       1.00      1.00      1.00     56746\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating Xgboost...\n",
            "==================================================\n",
            "\n",
            "Metrics for Xgboost:\n",
            "  Accuracy:  0.9989\n",
            "  Precision: 0.6441\n",
            "  Recall:    0.8000\n",
            "  F1-Score:  0.7136\n",
            "  ROC-AUC:   0.9736\n",
            "  PR-AUC:    0.8056\n",
            "\n",
            "Confusion Matrix:\n",
            "  TN: 56609, FP: 42\n",
            "  FN: 19, TP: 76\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      1.00      1.00     56651\n",
            "       Fraud       0.64      0.80      0.71        95\n",
            "\n",
            "    accuracy                           1.00     56746\n",
            "   macro avg       0.82      0.90      0.86     56746\n",
            "weighted avg       1.00      1.00      1.00     56746\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate each model\n",
        "for model_name, model in trained_models.items():\n",
        "    # Format model name for display\n",
        "    display_name = model_name.replace('_', ' ').title()\n",
        "    evaluator.evaluate_model(model, X_test, y_test, display_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "MODEL COMPARISON\n",
            "======================================================================\n",
            "              Model  Accuracy  Precision   Recall  F1-Score  ROC-AUC   PR-AUC\n",
            "      Random Forest  0.999489   0.985294 0.705263  0.822086 0.929913 0.807649\n",
            "            Xgboost  0.998925   0.644068 0.800000  0.713615 0.973605 0.805578\n",
            "      Decision Tree  0.995207   0.227692 0.778947  0.352381 0.888592 0.497137\n",
            "Logistic Regression  0.975047   0.055817 0.873684  0.104930 0.964832 0.675156\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>ROC-AUC</th>\n",
              "      <th>PR-AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.999489</td>\n",
              "      <td>0.985294</td>\n",
              "      <td>0.705263</td>\n",
              "      <td>0.822086</td>\n",
              "      <td>0.929913</td>\n",
              "      <td>0.807649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Xgboost</td>\n",
              "      <td>0.998925</td>\n",
              "      <td>0.644068</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.713615</td>\n",
              "      <td>0.973605</td>\n",
              "      <td>0.805578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.995207</td>\n",
              "      <td>0.227692</td>\n",
              "      <td>0.778947</td>\n",
              "      <td>0.352381</td>\n",
              "      <td>0.888592</td>\n",
              "      <td>0.497137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.975047</td>\n",
              "      <td>0.055817</td>\n",
              "      <td>0.873684</td>\n",
              "      <td>0.104930</td>\n",
              "      <td>0.964832</td>\n",
              "      <td>0.675156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model  Accuracy  Precision    Recall  F1-Score   ROC-AUC  \\\n",
              "2        Random Forest  0.999489   0.985294  0.705263  0.822086  0.929913   \n",
              "3              Xgboost  0.998925   0.644068  0.800000  0.713615  0.973605   \n",
              "0        Decision Tree  0.995207   0.227692  0.778947  0.352381  0.888592   \n",
              "1  Logistic Regression  0.975047   0.055817  0.873684  0.104930  0.964832   \n",
              "\n",
              "     PR-AUC  \n",
              "2  0.807649  \n",
              "3  0.805578  \n",
              "0  0.497137  \n",
              "1  0.675156  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compare all models\n",
        "comparison_df = evaluator.compare_models()\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC curves saved to ..\\reports\\roc_curves.png\n",
            "ROC curves saved to ../reports/roc_curves.png\n"
          ]
        }
      ],
      "source": [
        "# Plot ROC curves\n",
        "evaluator.plot_roc_curves(save_path='roc_curves.png')\n",
        "print(\"ROC curves saved to ../reports/roc_curves.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PR curves saved to ..\\reports\\pr_curves.png\n",
            "Precision-Recall curves saved to ../reports/pr_curves.png\n"
          ]
        }
      ],
      "source": [
        "# Plot PR curves\n",
        "evaluator.plot_pr_curves(save_path='pr_curves.png')\n",
        "print(\"Precision-Recall curves saved to ../reports/pr_curves.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrices saved to ..\\reports\\confusion_matrices.png\n",
            "Confusion matrices saved to ../reports/confusion_matrices.png\n"
          ]
        }
      ],
      "source": [
        "# Plot confusion matrices\n",
        "evaluator.plot_confusion_matrices(save_path='confusion_matrices.png')\n",
        "print(\"Confusion matrices saved to ../reports/confusion_matrices.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot metrics comparison\n",
        "evaluator.plot_metrics_comparison(save_path='metrics_comparison.png')\n",
        "print(\"Metrics comparison saved to ../reports/metrics_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "MODEL COMPARISON\n",
            "======================================================================\n",
            "              Model  Accuracy  Precision   Recall  F1-Score  ROC-AUC   PR-AUC\n",
            "      Random Forest  0.999489   0.985294 0.705263  0.822086 0.929913 0.807649\n",
            "            Xgboost  0.998925   0.644068 0.800000  0.713615 0.973605 0.805578\n",
            "      Decision Tree  0.995207   0.227692 0.778947  0.352381 0.888592 0.497137\n",
            "Logistic Regression  0.975047   0.055817 0.873684  0.104930 0.964832 0.675156\n",
            "Results saved to ..\\reports\\evaluation_results.csv\n",
            "\n",
            "Results saved to: ..\\reports\\evaluation_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Save evaluation results to CSV\n",
        "results_path = evaluator.save_results('evaluation_results.csv')\n",
        "print(f\"\\nResults saved to: {results_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
